{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import gym.spaces as spaces\n",
    "import gym.envs as envs\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/openai/gym/blob/master/gym/envs/toy_text/nchain.py\n",
    "# Customization to accomodate with the implementation\n",
    "N = 5\n",
    "env = gym.make('NChain-v0', n=N, large= 10, small = 2, slip = 0.2 )\n",
    "\n",
    "env.nS = env.observation_space.n\n",
    "env.nA = env.action_space.n\n",
    "\n",
    "# P[state][action] = prob, reward, s_prime\n",
    "env.P = np.zeros((env.nS, env.nA, env.nA, 3)) \n",
    "for s in range(env.nS):\n",
    "    if s == env.nS - 1 :\n",
    "        env.P[s][0] = [ [(1-env.slip), env.large, s], [env.slip, env.small, 0] ]\n",
    "        env.P[s][1] = [ [env.slip, env.large, s], [1 - env.slip, env.small, 0] ]\n",
    "        continue\n",
    "    # Forward\n",
    "    env.P[s][0] = [ [(1-env.slip), 0 if s < env.nS - 1 else env.large, s+1], [env.slip, env.small, 0] ]\n",
    "    # Backward\n",
    "    env.P[s][1] = [ [env.slip, 0 if s < env.nS - 1 else env.large, s+1], [1 - env.slip, env.small, 0] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_actions(optimal_policy):\n",
    "    policy = []\n",
    "    for i in optimal_policy:\n",
    "        if i == 0: # Forward\n",
    "            policy.append('F')\n",
    "        elif i == 1: # Backward\n",
    "            policy.append('B')\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Analysis\n",
    "\n",
    "def get_score(env, policy, max_timestep=200, episodes=1000):\n",
    "    R = np.zeros((episodes, max_timestep))\n",
    "    A = np.zeros((episodes, max_timestep))\n",
    "    ended = 0\n",
    "    reached = []\n",
    "    for ep in range(episodes):\n",
    "        s = env.reset()\n",
    "        for i in range(max_timestep):\n",
    "            a = policy[s]\n",
    "            A[ep][i] = a\n",
    "            s_prime, reward, _, _ = env.step(a)\n",
    "            R[ep][i] = reward\n",
    "            s = s_prime\n",
    "        if env.large in R[ep]:\n",
    "            ended += 1\n",
    "            reached.append(ep)\n",
    "    \n",
    "    total_R = np.sum(R, axis=1)\n",
    "    max_r = np.max(total_R)\n",
    "    min_r = np.min(total_R)\n",
    "    avg_r = np.mean(total_R)\n",
    "    print(\"Number of episodes reached end = \", ended, \" out of \", episodes, \" episodes.\" )\n",
    "    if len(reached) > 0:\n",
    "        print(\"Max reward where episode reached end = \", np.max(total_R[reached]))\n",
    "        print(\"Min reward where episode reached end = \", np.min(total_R[reached]))\n",
    "    print(\"Max Reward = \", max_r)\n",
    "    print(\"Avg Reward = \", avg_r)\n",
    "    print(\"Min Reward = \", min_r)\n",
    "    \n",
    "    return R, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "- Procedure Value_Iteration(S,A,P,R,θ):\n",
    "           Inputs\n",
    "                     S is the set of all states\n",
    "                     A is the set of all actions\n",
    "                     P is state transition function specifying P(s'|s,a)\n",
    "                     R is a reward function R(s,a,s')\n",
    "                     θ a threshold, θ>0\n",
    "           Output\n",
    "                     π[S] approximately optimal policy\n",
    "                    V[S] value function\n",
    "           Local\n",
    "                     real array Vk[S] is a sequence of value functions\n",
    "                     action array π[S]\n",
    "           assign V0[S] arbitrarily\n",
    "           k ←0\n",
    "           repeat\n",
    "                     k ←k+1\n",
    "                     for each state s do\n",
    "                               Vk[s] = maxa ∑s' P(s'|s,a) (R(s,a,s')+ γVk-1[s'])\n",
    "           until ∀s |Vk[s]-Vk-1[s]| < θ\n",
    "           for each state s do\n",
    "                     π[s] = argmaxa ∑s' P(s'|s,a) (R(s,a,s')+ γVk[s'])\n",
    "           return π,Vk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_values(env, s, V, gamma=0.99):\n",
    "    action_values = np.zeros(env.nA)\n",
    "    \n",
    "    for a in range(env.nA):\n",
    "        for prob, reward, s_prime in env.P[s][a]:\n",
    "            action_values[a] += prob * ( reward + gamma * V[int(s_prime)])\n",
    "            \n",
    "    return action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Value Iteration'''\n",
    "def value_iteration(env, gamma = 0.999, max_iteration = 1000):\n",
    "    # Initialise Utility Function\n",
    "    V = np.zeros(env.nS)\n",
    "\n",
    "    for i in range(max_iteration):\n",
    "        prev_V = np.copy(V)\n",
    "\n",
    "        #loop over all states\n",
    "        for s in range(env.nS):\n",
    "            action_values = get_action_values(env, s, prev_V, gamma)\n",
    "            best_action_value = np.max(action_values)\n",
    "            V[s] = best_action_value\n",
    "\n",
    "        if i%10 == 0 and np.all(np.isclose(V, prev_V)):\n",
    "            print(\"Value converged at iteration \", i)\n",
    "            break\n",
    "\n",
    "    optimal_policy = np.zeros(env.nS, dtype = 'int8')\n",
    "    for s in range(env.nS):\n",
    "        s_action_value = get_action_values(env, s, V, gamma)\n",
    "        optimal_policy[s] = np.argmax(s_action_value)\n",
    "\n",
    "    return V, optimal_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Value Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states:  2\n",
      "Number of actions:  5\n",
      "Value converged at iteration  4620\n",
      "Time to converge:  4.02e+02 ms\n",
      "Optimal Value function: \n",
      "[3627.62654429 3631.71026857 3636.82003369 3643.21363369 3651.21363369]\n",
      "Final Policy: \n",
      "['F', 'F', 'F', 'F', 'F']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "print(\"Number of states: \", env.action_space.n)\n",
    "print(\"Number of actions: \", env.observation_space.n)\n",
    "\n",
    "start_time = time.time()\n",
    "optimal_value, optimal_policy = value_iteration(env, gamma=0.999, max_iteration=10000 )\n",
    "stop_time = time.time()\n",
    "time_taken = (stop_time - start_time)*1000\n",
    "\n",
    "print (f\"Time to converge: {time_taken : 0.3} ms\")\n",
    "\n",
    "print('Optimal Value function: ')\n",
    "print(optimal_value)\n",
    "plt.imshow(optimal_value.reshape(1,N), cmap='gray')\n",
    "\n",
    "print('Final Policy: ')\n",
    "policy = map_actions(optimal_policy)\n",
    "\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of episodes reached end =  320  out of  1000  episodes.\n",
      "Max reward where episode reached end =  10.0\n",
      "Min reward where episode reached end =  10.0\n",
      "Max Reward =  10.0\n",
      "Avg Reward =  5.204\n",
      "Min Reward =  2.0\n",
      "Number of episodes reached end =  660  out of  1000  episodes.\n",
      "Max reward where episode reached end =  60.0\n",
      "Min reward where episode reached end =  12.0\n",
      "Max Reward =  60.0\n",
      "Avg Reward =  23.6\n",
      "Min Reward =  4.0\n",
      "Number of episodes reached end =  919  out of  1000  episodes.\n",
      "Max reward where episode reached end =  160.0\n",
      "Min reward where episode reached end =  18.0\n",
      "Max Reward =  160.0\n",
      "Avg Reward =  60.236\n",
      "Min Reward =  8.0\n"
     ]
    }
   ],
   "source": [
    "R, A = get_score(env, optimal_policy, max_timestep=5, episodes=1000)\n",
    "\n",
    "R, A = get_score(env, optimal_policy, max_timestep=10, episodes=1000)\n",
    "\n",
    "R, A = get_score(env, optimal_policy, max_timestep=20, episodes=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration\n",
    "The policy iteration algorithm manipulates the policy directly, rather than finding it indirectly via the optimal value function. It operates as follows:\n",
    "\n",
    "<img src='http://incompleteideas.net/book/first/ebook/pseudotmp1.png'>\n",
    "<img src='http://incompleteideas.net/book/first/ebook/imgtmp35.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy_val(env, policy, V, gamma):\n",
    "    policy_values = np.zeros(env.nS)\n",
    "    for s, a in zip(range(len(policy)), policy):\n",
    "        for prob, reward, s_prime in env.P[s][a]:\n",
    "            policy_values[s] += prob * (reward + gamma * V[int(s_prime)])\n",
    "            \n",
    "    return policy_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, gamma = 0.99, max_iteration = 1000):\n",
    "    V = np.zeros(env.nS)\n",
    "    \n",
    "    P = np.random.randint(0, env.nA, env.nS)\n",
    "    prev_P = np.copy(P)\n",
    "    \n",
    "    for i in range(max_iteration):\n",
    "        \n",
    "        V = get_policy_val(env, P, V, gamma)\n",
    "        \n",
    "        for s in range(env.nS):\n",
    "            s_action_value = get_action_values(env, s, V, gamma)\n",
    "            P[s] = np.argmax(s_action_value)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            if np.all(np.equal(P, prev_P)):\n",
    "                print(\"Policy converged at iteration \", i)\n",
    "                break\n",
    "            prev_P = np.copy(P)\n",
    "        \n",
    "    return V, P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states:  2\n",
      "Number of actions:  5\n",
      "Policy converged at iteration  20\n",
      "Time to converge:  3.16 ms\n",
      "Optimal Value function: \n",
      "[64.71380396 68.79752825 73.90729337 80.30089337 88.30089337]\n",
      "Final Policy: \n",
      "['F', 'F', 'F', 'F', 'F']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "print(\"Number of states: \", env.action_space.n)\n",
    "print(\"Number of actions: \", env.observation_space.n)\n",
    "\n",
    "start_time = time.time()\n",
    "optimal_value, optimal_policy = policy_iteration(env, gamma=0.999, max_iteration=1000)\n",
    "stop_time = time.time()\n",
    "time_taken = (stop_time - start_time)*1000\n",
    "\n",
    "print (f\"Time to converge: {time_taken : 0.3} ms\")\n",
    "\n",
    "print('Optimal Value function: ')\n",
    "print(optimal_value)\n",
    "plt.imshow(optimal_value.reshape((1, N)), cmap='gray')\n",
    "\n",
    "print('Final Policy: ')\n",
    "policy = map_actions(optimal_policy)\n",
    "\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of episodes reached end =  321  out of  1000  episodes.\n",
      "Max reward where episode reached end =  10.0\n",
      "Min reward where episode reached end =  10.0\n",
      "Max Reward =  10.0\n",
      "Avg Reward =  5.204\n",
      "Min Reward =  2.0\n",
      "Number of episodes reached end =  654  out of  1000  episodes.\n",
      "Max reward where episode reached end =  60.0\n",
      "Min reward where episode reached end =  12.0\n",
      "Max Reward =  60.0\n",
      "Avg Reward =  23.678\n",
      "Min Reward =  4.0\n",
      "Number of episodes reached end =  926  out of  1000  episodes.\n",
      "Max reward where episode reached end =  160.0\n",
      "Min reward where episode reached end =  16.0\n",
      "Max Reward =  160.0\n",
      "Avg Reward =  58.96\n",
      "Min Reward =  8.0\n"
     ]
    }
   ],
   "source": [
    "R, A = get_score(env, optimal_policy, max_timestep=5, episodes=1000)\n",
    "\n",
    "R, A = get_score(env, optimal_policy, max_timestep=10, episodes=1000)\n",
    "\n",
    "R, A = get_score(env, optimal_policy, max_timestep=20, episodes=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q- Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "env.reset()\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "\n",
    "qtable = np.zeros((state_size, action_size))\n",
    "T = np.zeros((state_size, action_size, state_size))\n",
    "T_Count = T.copy()\n",
    "R = qtable.copy()\n",
    "\n",
    "total_episodes = 100000       # Total episodes\n",
    "learning_rate = 0.8           # Learning rate\n",
    "max_steps = 99                # Max steps per episode\n",
    "gamma = 0.95                  # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 189.8745\n",
      "[[20.77464822 30.2851811 ]\n",
      " [33.16380003 21.33300923]\n",
      " [36.04462851 21.83521479]\n",
      " [36.36023094 22.39171522]\n",
      " [37.06470044 22.96625699]]\n"
     ]
    }
   ],
   "source": [
    "# List of rewards\n",
    "rewards = []\n",
    "\n",
    "# 2 For life or until learning is stopped\n",
    "for episode in range(total_episodes):\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # 3. Choose an action a in the current world state (s)\n",
    "        ## First we randomize a number\n",
    "        exp_exp_tradeoff = random.uniform(0, 1)\n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable[state,:])\n",
    "\n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        # qtable[new_state,:] : all the actions we can take from new state\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "                \n",
    "        total_rewards += reward\n",
    "        \n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "        \n",
    "        # If done (if we're dead) : finish episode\n",
    "        if done == True: \n",
    "            break\n",
    "        \n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value function: \n",
      "[30.285 33.164 36.045 36.36  37.065]\n",
      "Final Policy: \n",
      "['B' 'F' 'F' 'F' 'F']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimal_policy = np.argmax(qtable, axis=1)\n",
    "optimal_value = np.amax(qtable, axis=1)\n",
    "\n",
    "print('Optimal Value function: ')\n",
    "print(np.round(optimal_value, 3))\n",
    "plt.imshow(optimal_value.reshape((1, N)), cmap='gray')\n",
    "\n",
    "print('Final Policy: ')\n",
    "policy = map_actions(optimal_policy)\n",
    "\n",
    "print(np.array(policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of episodes reached end =  82  out of  1000  episodes.\n",
      "Max reward where episode reached end =  10.0\n",
      "Min reward where episode reached end =  10.0\n",
      "Max Reward =  10.0\n",
      "Avg Reward =  7.224\n",
      "Min Reward =  2.0\n",
      "Number of episodes reached end =  342  out of  1000  episodes.\n",
      "Max reward where episode reached end =  60.0\n",
      "Min reward where episode reached end =  12.0\n",
      "Max Reward =  60.0\n",
      "Avg Reward =  20.844\n",
      "Min Reward =  4.0\n",
      "Number of episodes reached end =  664  out of  1000  episodes.\n",
      "Max reward where episode reached end =  160.0\n",
      "Min reward where episode reached end =  22.0\n",
      "Max Reward =  160.0\n",
      "Avg Reward =  51.144\n",
      "Min Reward =  16.0\n"
     ]
    }
   ],
   "source": [
    "R, A = get_score(env, optimal_policy, max_timestep=5, episodes=1000)\n",
    "\n",
    "R, A = get_score(env, optimal_policy, max_timestep=10, episodes=1000)\n",
    "\n",
    "R, A = get_score(env, optimal_policy, max_timestep=20, episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "env.reset()\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "\n",
    "qtable = np.zeros((state_size, action_size))\n",
    "T = np.zeros((state_size, action_size, state_size))\n",
    "T_Count = T.copy()\n",
    "R = qtable.copy()\n",
    "\n",
    "total_episodes = 100000       # Total episodes\n",
    "learning_rate = 0.8           # Learning rate\n",
    "max_steps = 99                # Max steps per episode\n",
    "gamma = 0.95                  # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 195.95084\n",
      "[[23.51343068 24.99559788]\n",
      " [23.76599309 25.23384452]\n",
      " [24.10012463 24.80065067]\n",
      " [24.48045672 25.91289564]\n",
      " [34.23884721 23.55517845]]\n"
     ]
    }
   ],
   "source": [
    "# List of rewards\n",
    "rewards = []\n",
    "\n",
    "# 2 For life or until learning is stopped\n",
    "for episode in range(total_episodes):\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # 3. Choose an action a in the current world state (s)\n",
    "        ## First we randomize a number\n",
    "        exp_exp_tradeoff = random.uniform(0, 1)\n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable[state,:])\n",
    "\n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        # qtable[new_state,:] : all the actions we can take from new state\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "                \n",
    "        total_rewards += reward\n",
    "        \n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "        \n",
    "        # If done (if we're dead) : finish episode\n",
    "        if done == True: \n",
    "            break\n",
    "        \n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value function: \n",
      "[24.996 25.234 24.801 25.913 34.239]\n",
      "Final Policy: \n",
      "['B' 'B' 'B' 'B' 'F']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimal_policy = np.argmax(qtable, axis=1)\n",
    "optimal_value = np.amax(qtable, axis=1)\n",
    "\n",
    "print('Optimal Value function: ')\n",
    "print(np.round(optimal_value, 3))\n",
    "plt.imshow(optimal_value.reshape((1, N)), cmap='gray')\n",
    "\n",
    "print('Final Policy: ')\n",
    "policy = map_actions(optimal_policy)\n",
    "\n",
    "print(np.array(policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of episodes reached end =  2  out of  1000  episodes.\n",
      "Max reward where episode reached end =  10.0\n",
      "Min reward where episode reached end =  10.0\n",
      "Max Reward =  10.0\n",
      "Avg Reward =  7.958\n",
      "Min Reward =  2.0\n",
      "Number of episodes reached end =  7  out of  1000  episodes.\n",
      "Max reward where episode reached end =  44.0\n",
      "Min reward where episode reached end =  16.0\n",
      "Max Reward =  44.0\n",
      "Avg Reward =  16.076\n",
      "Min Reward =  8.0\n",
      "Number of episodes reached end =  16  out of  1000  episodes.\n",
      "Max reward where episode reached end =  128.0\n",
      "Min reward where episode reached end =  30.0\n",
      "Max Reward =  128.0\n",
      "Avg Reward =  32.718\n",
      "Min Reward =  22.0\n"
     ]
    }
   ],
   "source": [
    "R, A = get_score(env, optimal_policy, max_timestep=5, episodes=1000)\n",
    "\n",
    "R, A = get_score(env, optimal_policy, max_timestep=10, episodes=1000)\n",
    "\n",
    "R, A = get_score(env, optimal_policy, max_timestep=20, episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "env.reset()\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "\n",
    "qtable = np.zeros((state_size, action_size))\n",
    "T = np.zeros((state_size, action_size, state_size))\n",
    "T_Count = T.copy()\n",
    "R = qtable.copy()\n",
    "\n",
    "total_episodes = 100000       # Total episodes\n",
    "learning_rate = 0.8           # Learning rate\n",
    "max_steps = 99                # Max steps per episode\n",
    "gamma = 0.95                  # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 194.22406\n",
      "[[24.84671367 24.96088044]\n",
      " [24.05420865 25.78158028]\n",
      " [27.96629565 24.50453178]\n",
      " [26.45767078 21.0294546 ]\n",
      " [29.66170942 24.29739073]]\n"
     ]
    }
   ],
   "source": [
    "# List of rewards\n",
    "rewards = []\n",
    "\n",
    "# 2 For life or until learning is stopped\n",
    "for episode in range(total_episodes):\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # 3. Choose an action a in the current world state (s)\n",
    "        ## First we randomize a number\n",
    "        exp_exp_tradeoff = random.uniform(0, 1)\n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable[state,:])\n",
    "\n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        # qtable[new_state,:] : all the actions we can take from new state\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "                \n",
    "        total_rewards += reward\n",
    "        \n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "        \n",
    "        # If done (if we're dead) : finish episode\n",
    "        if done == True: \n",
    "            break\n",
    "        \n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value function: \n",
      "[24.961 25.782 27.966 26.458 29.662]\n",
      "Final Policy: \n",
      "['B' 'B' 'F' 'F' 'F']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimal_policy = np.argmax(qtable, axis=1)\n",
    "optimal_value = np.amax(qtable, axis=1)\n",
    "\n",
    "print('Optimal Value function: ')\n",
    "print(np.round(optimal_value, 3))\n",
    "plt.imshow(optimal_value.reshape((1, N)), cmap='gray')\n",
    "\n",
    "print('Final Policy: ')\n",
    "policy = map_actions(optimal_policy)\n",
    "\n",
    "print(np.array(policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of episodes reached end =  25  out of  1000  episodes.\n",
      "Max reward where episode reached end =  10.0\n",
      "Min reward where episode reached end =  10.0\n",
      "Max Reward =  10.0\n",
      "Avg Reward =  8.008\n",
      "Min Reward =  2.0\n",
      "Number of episodes reached end =  97  out of  1000  episodes.\n",
      "Max reward where episode reached end =  60.0\n",
      "Min reward where episode reached end =  16.0\n",
      "Max Reward =  60.0\n",
      "Avg Reward =  17.75\n",
      "Min Reward =  6.0\n",
      "Number of episodes reached end =  236  out of  1000  episodes.\n",
      "Max reward where episode reached end =  144.0\n",
      "Min reward where episode reached end =  26.0\n",
      "Max Reward =  144.0\n",
      "Avg Reward =  38.988\n",
      "Min Reward =  20.0\n"
     ]
    }
   ],
   "source": [
    "R, A = get_score(env, optimal_policy, max_timestep=5, episodes=1000)\n",
    "\n",
    "R, A = get_score(env, optimal_policy, max_timestep=10, episodes=1000)\n",
    "\n",
    "R, A = get_score(env, optimal_policy, max_timestep=20, episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
