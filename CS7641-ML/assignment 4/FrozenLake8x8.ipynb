{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import gym.spaces as spaces\n",
    "import gym.envs as envs\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Analysis\n",
    "\n",
    "def get_score(env, policy, episodes=1000):\n",
    "    wins = 0\n",
    "    loses = 0\n",
    "    total_reward = 0\n",
    "    steps = []\n",
    "    sequence = []\n",
    "    min_steps = np.Inf\n",
    "    \n",
    "    for i in range(episodes):\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        cnt = 0\n",
    "        seq_actions = []\n",
    "        while True:\n",
    "            action = policy[state]\n",
    "            seq_actions.append(action)\n",
    "\n",
    "            s_prime, r, done, _ = env.step(action)\n",
    "            \n",
    "            total_reward += r\n",
    "            cnt += 1\n",
    "            state = s_prime\n",
    "            \n",
    "            if done and r == 1.0:\n",
    "                wins += 1\n",
    "                steps.append(cnt)\n",
    "                if cnt < min_steps:\n",
    "                    min_steps = cnt\n",
    "                    sequence = seq_actions\n",
    "                break;\n",
    "            elif done and r == 0.0:\n",
    "                loses += 1\n",
    "                break;\n",
    "    \n",
    "    print(\"Won Percentage = \", (wins/episodes) * 100)\n",
    "    print(\"Lost Percentage = \", (loses/episodes) * 100)\n",
    "    print(\"Average steps taken to win = \", np.mean(steps))\n",
    "    print(\"Average reward in all episodes = \", total_reward/episodes)\n",
    "    \n",
    "    return wins, loses, steps, sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_actions(optimal_policy):\n",
    "    policy = []\n",
    "    for i in optimal_policy:\n",
    "        if i == 0: # LEFT\n",
    "            policy.append('\\u2190')\n",
    "        elif i == 1: # DOWN\n",
    "            policy.append('\\u2193')\n",
    "        elif i == 2: # RIGHT\n",
    "            policy.append('\\u2192')\n",
    "        elif i == 3: # UP\n",
    "            policy.append('\\u2191')\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "- Procedure Value_Iteration(S,A,P,R,θ):\n",
    "           Inputs\n",
    "                     S is the set of all states\n",
    "                     A is the set of all actions\n",
    "                     P is state transition function specifying P(s'|s,a)\n",
    "                     R is a reward function R(s,a,s')\n",
    "                     θ a threshold, θ>0\n",
    "           Output\n",
    "                     π[S] approximately optimal policy\n",
    "                    V[S] value function\n",
    "           Local\n",
    "                     real array Vk[S] is a sequence of value functions\n",
    "                     action array π[S]\n",
    "           assign V0[S] arbitrarily\n",
    "           k ←0\n",
    "           repeat\n",
    "                     k ←k+1\n",
    "                     for each state s do\n",
    "                               Vk[s] = maxa ∑s' P(s'|s,a) (R(s,a,s')+ γVk-1[s'])\n",
    "           until ∀s |Vk[s]-Vk-1[s]| < θ\n",
    "           for each state s do\n",
    "                     π[s] = argmaxa ∑s' P(s'|s,a) (R(s,a,s')+ γVk[s'])\n",
    "           return π,Vk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_values(env, s, V, gamma=0.99):\n",
    "    action_values = np.zeros(env.nA)\n",
    "    \n",
    "    for a in range(env.nA):\n",
    "        for prob, s_prime, r, _ in env.P[s][a]:\n",
    "            action_values[a] += prob * ( r + gamma * V[s_prime])\n",
    "            \n",
    "    return action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Value Iteration'''\n",
    "def value_iteration(env, gamma = 0.999, max_iteration = 1000):\n",
    "    # Initialise Utility Function\n",
    "    V = np.zeros(env.nS)\n",
    "\n",
    "    for i in range(max_iteration):\n",
    "        prev_V = np.copy(V)\n",
    "\n",
    "        #loop over all states\n",
    "        for s in range(env.nS):\n",
    "            action_values = get_action_values(env, s, prev_V, gamma)\n",
    "            best_action_value = np.max(action_values)\n",
    "            V[s] = best_action_value\n",
    "\n",
    "        if i % 5 == 0 and np.all(np.isclose(V, prev_V)):\n",
    "            print(\"Value converged at iteration \", i)\n",
    "            break\n",
    "\n",
    "    optimal_policy = np.zeros(env.nS, dtype = 'int8')\n",
    "    for s in range(env.nS):\n",
    "        s_action_value = get_action_values(env, s, V, gamma)\n",
    "        optimal_policy[s] = np.argmax(s_action_value)\n",
    "\n",
    "    return V, optimal_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Value Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states:  64\n",
      "Number of actions:  4\n",
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "Value converged at iteration  595\n",
      "Time to converge:  5.7e+02 ms\n",
      "Optimal Value function: \n",
      "[[0.893 0.895 0.899 0.904 0.909 0.914 0.919 0.922]\n",
      " [0.892 0.894 0.897 0.902 0.906 0.912 0.917 0.925]\n",
      " [0.876 0.862 0.819 0.    0.778 0.865 0.909 0.931]\n",
      " [0.864 0.811 0.699 0.42  0.564 0.    0.881 0.939]\n",
      " [0.853 0.711 0.469 0.    0.494 0.568 0.799 0.95 ]\n",
      " [0.846 0.    0.    0.153 0.353 0.413 0.    0.964]\n",
      " [0.841 0.    0.164 0.105 0.    0.319 0.    0.981]\n",
      " [0.838 0.612 0.387 0.    0.272 0.544 0.772 0.   ]]\n",
      "Final Policy: \n",
      "[['↑' '→' '→' '→' '→' '→' '→' '→']\n",
      " ['↑' '↑' '↑' '↑' '↑' '↑' '↑' '→']\n",
      " ['←' '↑' '←' '←' '→' '↑' '→' '→']\n",
      " ['←' '←' '←' '↓' '←' '←' '→' '→']\n",
      " ['←' '↑' '←' '←' '→' '↓' '↑' '→']\n",
      " ['←' '←' '←' '↓' '↑' '←' '←' '→']\n",
      " ['←' '←' '↓' '←' '←' '←' '←' '→']\n",
      " ['←' '↓' '←' '←' '↓' '→' '↓' '←']]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('FrozenLake8x8-v0')\n",
    "print(\"Number of states: \", env.nS)\n",
    "print(\"Number of actions: \", env.nA)\n",
    "\n",
    "env.render()\n",
    "\n",
    "start_time = time.time()\n",
    "optimal_value, optimal_policy = value_iteration(env.env, gamma=0.999, max_iteration=1000 )\n",
    "stop_time = time.time()\n",
    "time_taken = (stop_time - start_time)*1000\n",
    "\n",
    "print (f\"Time to converge: {time_taken : 0.3} ms\")\n",
    "\n",
    "print('Optimal Value function: ')\n",
    "print(np.round(optimal_value, 3).reshape((8, 8)))\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(optimal_value.reshape((8, 8)), cmap='Oranges_r')\n",
    "\n",
    "print('Final Policy: ')\n",
    "policy = map_actions(optimal_policy)\n",
    "\n",
    "print(np.array(policy).reshape((8,8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Won Percentage =  88.1\n",
      "Lost Percentage =  11.899999999999999\n",
      "Average steps taken to win =  98.07150964812713\n",
      "Average reward in all episodes =  0.881\n",
      "Sequence of min steps taken:  ['↑', '→', '→', '→', '→', '↑', '↑', '↑', '↑', '→', '→', '↑', '→', '↑', '→', '→', '→', '→', '→', '→', '→']\n",
      "Min steps to win:  21\n"
     ]
    }
   ],
   "source": [
    "wins, loses, steps, sequence = get_score(env, optimal_policy, episodes=1000)\n",
    "print(\"Sequence of min steps taken: \", map_actions(sequence))\n",
    "print(\"Min steps to win: \", len(sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration\n",
    "The policy iteration algorithm manipulates the policy directly, rather than finding it indirectly via the optimal value function. It operates as follows:\n",
    "\n",
    "<img src='http://incompleteideas.net/book/first/ebook/pseudotmp1.png'>\n",
    "<img src='http://incompleteideas.net/book/first/ebook/imgtmp35.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy_val(env, policy, V, gamma):\n",
    "    policy_values = np.zeros(env.nS)\n",
    "    for s, a in zip(range(len(policy)), policy):\n",
    "        for prob, s_prime, r, _ in env.P[s][a]:\n",
    "            policy_values[s] += prob * ( r + gamma * V[s_prime])\n",
    "            \n",
    "    return policy_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, gamma = 0.99, max_iteration = 1000):\n",
    "    V = np.zeros(env.nS)\n",
    "    \n",
    "    P = np.random.randint(0, 4, env.nS)\n",
    "    prev_P = np.copy(P)\n",
    "    \n",
    "    for i in range(max_iteration):\n",
    "        \n",
    "        V = get_policy_val(env, P, V, gamma)\n",
    "        \n",
    "        for s in range(env.nS):\n",
    "            s_action_value = get_action_values(env, s, V, gamma)\n",
    "            P[s] = np.argmax(s_action_value)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            if np.all(np.equal(P, prev_P)):\n",
    "                print(\"Policy converged at iteration \", i)\n",
    "                break\n",
    "            prev_P = np.copy(P)\n",
    "        \n",
    "    return V, P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states:  64\n",
      "Number of actions:  4\n",
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "Policy converged at iteration  70\n",
      "Time to converge:  79.2 ms\n",
      "Optimal Value function: \n",
      "[[0.409 0.435 0.472 0.513 0.553 0.59  0.613 0.62 ]\n",
      " [0.402 0.422 0.455 0.494 0.537 0.585 0.625 0.638]\n",
      " [0.379 0.382 0.37  0.    0.468 0.559 0.641 0.668]\n",
      " [0.343 0.332 0.295 0.203 0.325 0.    0.646 0.711]\n",
      " [0.291 0.261 0.181 0.    0.317 0.403 0.602 0.768]\n",
      " [0.224 0.    0.    0.09  0.233 0.299 0.    0.837]\n",
      " [0.181 0.    0.034 0.041 0.    0.268 0.    0.916]\n",
      " [0.159 0.104 0.064 0.    0.254 0.509 0.754 0.   ]]\n",
      "Final Policy: \n",
      "[['↑' '→' '→' '→' '→' '→' '→' '→']\n",
      " ['↑' '↑' '↑' '↑' '↑' '→' '→' '↓']\n",
      " ['↑' '↑' '←' '←' '→' '↑' '→' '↓']\n",
      " ['↑' '↑' '↑' '↓' '←' '←' '→' '→']\n",
      " ['↑' '↑' '←' '←' '→' '↓' '↑' '→']\n",
      " ['←' '←' '←' '↓' '↑' '←' '←' '→']\n",
      " ['←' '←' '↓' '←' '←' '←' '←' '→']\n",
      " ['←' '↓' '←' '←' '↓' '→' '↓' '←']]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env2 = gym.make('FrozenLake8x8-v0')\n",
    "print(\"Number of states: \", env2.nS)\n",
    "print(\"Number of actions: \", env2.nA)\n",
    "\n",
    "env2.render()\n",
    "\n",
    "start_time2 = time.time()\n",
    "optimal_value2, optimal_policy2 = policy_iteration(env2.env, gamma=0.999, max_iteration=1000)\n",
    "stop_time2 = time.time()\n",
    "time_taken2 = (stop_time2 - start_time2) * 1000\n",
    "\n",
    "print (f\"Time to converge: {time_taken2 : 0.3} ms\")\n",
    "\n",
    "print('Optimal Value function: ')\n",
    "print(np.round(optimal_value2, 3).reshape((8, 8)))\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(optimal_value2.reshape((8, 8)), cmap='Oranges_r')\n",
    "\n",
    "print('Final Policy: ')\n",
    "policy2 = map_actions(optimal_policy2)\n",
    "\n",
    "print(np.array(policy2).reshape((8,8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Won Percentage =  85.8\n",
      "Lost Percentage =  14.2\n",
      "Average steps taken to win =  83.35664335664336\n",
      "Average reward in all episodes =  0.858\n",
      "Sequence of min steps taken:  ['↑', '↑', '→', '→', '→', '↑', '↑', '↑', '→', '↑', '→', '→', '↑', '→', '→', '→', '→', '→', '→', '→']\n",
      "Min steps to win:  20\n"
     ]
    }
   ],
   "source": [
    "wins2, loses2, steps2, sequence2 = get_score(env2, optimal_policy2, episodes=1000)\n",
    "print(\"Sequence of min steps taken: \", map_actions(sequence2))\n",
    "print(\"Min steps to win: \", len(sequence2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  1,  1,  1],\n",
       "       [-3,  0,  0,  0,  0,  0,  0,  1],\n",
       "       [-3, -3, -3,  0,  0,  0,  0,  0],\n",
       "       [-3,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(optimal_policy - optimal_policy2).reshape((8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q- Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "env3 = gym.make('FrozenLake8x8-v0')\n",
    "action_size = env3.action_space.n\n",
    "state_size = env3.observation_space.n\n",
    "\n",
    "qtable = np.zeros((state_size, action_size))\n",
    "T = np.zeros((state_size, action_size, state_size))\n",
    "T_Count = T.copy()\n",
    "R = qtable.copy()\n",
    "\n",
    "total_episodes = 100000        # Total episodes\n",
    "learning_rate = 0.8           # Learning rate\n",
    "max_steps = 99                # Max steps per episode\n",
    "gamma = 0.95                  # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.19433\n",
      "Time taken =  133.72452592849731  s.\n",
      "[[7.91717559e-03 2.34324691e-02 8.03674444e-03 7.86281715e-03]\n",
      " [9.06851192e-03 8.81200675e-03 1.62411374e-02 8.60271843e-03]\n",
      " [1.08637520e-02 1.31208155e-02 2.68929311e-02 1.06615104e-02]\n",
      " [3.41960677e-02 2.54091721e-02 1.68848051e-02 1.79127239e-02]\n",
      " [2.44756361e-02 2.73740152e-02 3.02835544e-02 4.06613199e-02]\n",
      " [2.74598249e-02 2.90626773e-02 4.86748408e-02 6.09468835e-02]\n",
      " [3.73733135e-02 6.42821678e-02 7.37696069e-02 3.85897182e-02]\n",
      " [3.83078563e-02 9.94297087e-02 5.20222425e-02 2.61381759e-02]\n",
      " [8.06037314e-03 6.98438959e-03 7.89687648e-03 1.69285323e-02]\n",
      " [4.72356877e-03 7.27395264e-03 8.36451380e-03 1.42152846e-02]\n",
      " [1.29423827e-02 9.83452446e-03 1.16337931e-02 2.22245885e-02]\n",
      " [7.57941108e-03 6.51311955e-04 5.89301053e-03 3.86352416e-02]\n",
      " [1.06298858e-02 6.88333509e-03 5.72484916e-02 2.65650875e-02]\n",
      " [1.56645192e-02 1.42817657e-02 2.82321778e-02 1.30092696e-01]\n",
      " [3.79073119e-02 5.99373863e-02 9.87537242e-02 2.88067508e-02]\n",
      " [4.62123846e-02 7.88451605e-02 1.31879008e-01 4.26682046e-02]\n",
      " [4.59023472e-03 5.50359113e-03 6.72420403e-03 5.58923157e-03]\n",
      " [3.11546834e-03 1.82781713e-03 6.83474850e-03 3.04848815e-03]\n",
      " [2.39822446e-03 4.09477982e-04 1.27234084e-03 1.47843689e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [3.02076778e-03 3.02743875e-03 3.18907090e-02 2.87359694e-03]\n",
      " [1.05313549e-03 3.21087634e-03 1.76621892e-03 7.19323921e-02]\n",
      " [2.17458949e-02 4.81873553e-02 2.40621018e-01 4.25755282e-02]\n",
      " [1.49609350e-01 6.67848820e-02 6.87202370e-02 6.90940744e-02]\n",
      " [1.84662442e-03 1.82958997e-03 3.87484964e-03 5.05021236e-03]\n",
      " [2.03572096e-03 4.25491058e-04 1.86267733e-03 4.20871865e-03]\n",
      " [2.68986347e-04 8.93806773e-05 3.59770797e-04 1.94263369e-04]\n",
      " [2.17714001e-09 6.16827133e-03 6.65706144e-10 2.62620605e-06]\n",
      " [5.35250877e-02 2.79961893e-05 1.05839876e-04 3.51461354e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.16222507e-02 1.75076503e-02 9.93172478e-02 1.28498508e-02]\n",
      " [7.41183251e-02 2.20636914e-01 9.83079969e-02 7.95578957e-02]\n",
      " [1.18763865e-03 2.09304248e-03 1.10060869e-03 2.70162964e-03]\n",
      " [4.22893608e-04 9.52340639e-05 2.85382439e-04 5.79306804e-04]\n",
      " [1.28938964e-05 2.73743192e-08 8.12979031e-12 2.23625141e-08]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.23026425e-03 5.57605527e-04 9.90971939e-03 5.19063096e-04]\n",
      " [2.26146404e-04 3.83970562e-02 5.02050725e-04 3.43099533e-04]\n",
      " [9.31751157e-06 3.45846842e-03 6.43364225e-05 4.20634357e-01]\n",
      " [5.98245446e-02 8.42747454e-02 4.83086755e-01 7.27182063e-02]\n",
      " [4.93174544e-03 1.82294527e-04 2.22294992e-04 2.48345327e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.86708084e-09 3.27957737e-08 1.48929799e-07 6.83105771e-10]\n",
      " [4.91061425e-05 4.08027361e-03 1.22523710e-06 1.29533612e-04]\n",
      " [2.50612012e-01 2.49648152e-04 4.90224728e-04 3.89392887e-05]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [3.12637146e-02 1.01646850e-01 6.81432864e-01 9.90179546e-02]\n",
      " [5.71016697e-04 2.18002994e-05 1.08296102e-04 1.33156831e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [6.43891950e-08 1.77432784e-07 1.95586003e-07 7.18066859e-08]\n",
      " [2.99839784e-08 4.21807428e-09 3.39090854e-09 7.73735101e-07]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [2.99860434e-05 1.23721828e-08 5.76030752e-01 1.91025792e-07]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.23397003e-01 3.83303911e-02 9.55065542e-01 5.70282939e-03]\n",
      " [5.69754810e-04 2.05266445e-04 2.00212675e-04 2.02313899e-04]\n",
      " [2.65498580e-06 8.06387967e-04 2.18145046e-05 9.87396376e-06]\n",
      " [4.82820230e-04 1.41486822e-06 2.59723584e-06 1.16195984e-05]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 9.66960994e-04 8.58975703e-02 0.00000000e+00]\n",
      " [6.16663137e-02 5.84321520e-02 8.58434444e-01 1.35897783e-02]\n",
      " [1.41565383e-01 9.82937840e-01 2.97422808e-02 1.84896041e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# List of rewards\n",
    "rewards = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 2 For life or until learning is stopped\n",
    "for episode in range(total_episodes):\n",
    "    # Reset the environment\n",
    "    state = env3.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # 3. Choose an action a in the current world state (s)\n",
    "        ## First we randomize a number\n",
    "        exp_exp_tradeoff = random.uniform(0, 1)\n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable[state,:])\n",
    "\n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = env3.action_space.sample()\n",
    "\n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env3.step(action)\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        # qtable[new_state,:] : all the actions we can take from new state\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "        \n",
    "        total_rewards += reward\n",
    "        \n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "        \n",
    "        # If done (if we're dead) : finish episode\n",
    "        if done == True: \n",
    "            break\n",
    "        \n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "stop_time = time.time()\n",
    "\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
    "print(\"Time taken = \", (stop_time - start_time), \" s.\")\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value function: \n",
      "[[0.023 0.016 0.027 0.034 0.041 0.061 0.074 0.099]\n",
      " [0.017 0.014 0.022 0.039 0.057 0.13  0.099 0.132]\n",
      " [0.007 0.007 0.002 0.    0.032 0.072 0.241 0.15 ]\n",
      " [0.005 0.004 0.    0.006 0.054 0.    0.099 0.221]\n",
      " [0.003 0.001 0.    0.    0.01  0.038 0.421 0.483]\n",
      " [0.005 0.    0.    0.    0.004 0.251 0.    0.681]\n",
      " [0.001 0.    0.    0.    0.    0.576 0.    0.955]\n",
      " [0.001 0.001 0.    0.    0.086 0.858 0.983 0.   ]]\n",
      "Final Policy: \n",
      "[['↓' '→' '→' '←' '↑' '↑' '→' '↓']\n",
      " ['↑' '↑' '↑' '↑' '→' '↑' '→' '→']\n",
      " ['→' '→' '←' '←' '→' '↑' '→' '←']\n",
      " ['↑' '↑' '→' '↓' '←' '←' '→' '↓']\n",
      " ['↑' '↑' '←' '←' '→' '↓' '↑' '→']\n",
      " ['←' '←' '←' '→' '↓' '←' '←' '→']\n",
      " ['←' '←' '→' '↑' '←' '→' '←' '→']\n",
      " ['←' '↓' '←' '←' '→' '→' '↓' '←']]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimal_policy3 = np.argmax(qtable, axis=1)\n",
    "optimal_value3 = np.amax(qtable, axis=1)\n",
    "\n",
    "print('Optimal Value function: ')\n",
    "print(np.round(optimal_value3, 3).reshape((8, 8)))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(optimal_value3.reshape((8, 8)), cmap='Oranges_r')\n",
    "\n",
    "print('Final Policy: ')\n",
    "policy3 = map_actions(optimal_policy3)\n",
    "\n",
    "print(np.array(policy3).reshape((8,8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Won Percentage =  59.099999999999994\n",
      "Lost Percentage =  40.9\n",
      "Average steps taken to win =  98.6429780033841\n",
      "Average reward in all episodes =  0.591\n",
      "Sequence of min steps taken:  ['↓', '↓', '→', '→', '→', '→', '→', '↑', '↑', '→', '→', '←', '→', '↓', '↑', '→', '→', '→', '→']\n",
      "Min steps to win:  19\n"
     ]
    }
   ],
   "source": [
    "wins3, loses3, steps3, sequence3 = get_score(env3, optimal_policy3, episodes=1000)\n",
    "print(\"Sequence of min steps taken: \", map_actions(sequence3))\n",
    "print(\"Min steps to win: \", len(sequence3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(np.round(optimal_value2, 3).reshape((8,8))).to_csv('a.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
