{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import gym.spaces as spaces\n",
    "import gym.envs as envs\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting of Environment\n",
    "N = 1000\n",
    "env = gym.make('NChain-v0', n=N, large=2000, small=2, slip=0.2)\n",
    "\n",
    "env.nS = env.observation_space.n\n",
    "env.nA = env.action_space.n\n",
    "\n",
    "# P[state][action] = prob, reward, s_prime\n",
    "env.P = np.zeros((env.nS, env.nA, env.nA, 3)) \n",
    "for s in range(env.nS):\n",
    "    if s == env.nS - 1 :\n",
    "        env.P[s][0] = [ [(1-env.slip), env.large, s], [env.slip, env.small, 0] ]\n",
    "        env.P[s][1] = [ [env.slip, env.large, s], [1 - env.slip, env.small, 0] ]\n",
    "        continue\n",
    "    # Forward\n",
    "    env.P[s][0] = [ [(1-env.slip), 0 if s < env.nS - 1 else env.large, s+1], [env.slip, env.small, 0] ]\n",
    "    # Backward\n",
    "    env.P[s][1] = [ [env.slip, 0 if s < env.nS - 1 else env.large, s+1], [1 - env.slip, env.small, 0] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_actions(optimal_policy):\n",
    "    policy = []\n",
    "    for i in optimal_policy:\n",
    "        if i == 0: # Forward\n",
    "            policy.append('F')\n",
    "        elif i == 1: # Backward\n",
    "            policy.append('B')\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Analysis\n",
    "\n",
    "def get_score(env, policy, max_timestep=200, episodes=1000):\n",
    "    R = np.zeros((episodes, max_timestep))\n",
    "    A = np.zeros((episodes, max_timestep))\n",
    "    ended = 0\n",
    "    reached = []\n",
    "    for ep in range(episodes):\n",
    "        s = env.reset()\n",
    "        for i in range(max_timestep):\n",
    "            a = policy[s]\n",
    "            A[ep][i] = a\n",
    "            s_prime, reward, _, _ = env.step(a)\n",
    "            R[ep][i] = reward\n",
    "            s = s_prime\n",
    "        if env.large in R[ep]:\n",
    "            ended += 1\n",
    "            reached.append(ep)\n",
    "    \n",
    "    total_R = np.sum(R, axis=1)\n",
    "    max_r = np.max(total_R)\n",
    "    min_r = np.min(total_R)\n",
    "    avg_r = np.mean(total_R)\n",
    "    print(\"Number of episodes reached end = \", ended, \" out of \", episodes, \" episodes.\" )\n",
    "    if len(reached) > 0:\n",
    "        print(\"Max reward where episode reached end = \", np.max(total_R[reached]))\n",
    "        print(\"Min reward where episode reached end = \", np.min(total_R[reached]))\n",
    "    print(\"Max Reward = \", max_r)\n",
    "    print(\"Avg Reward = \", avg_r)\n",
    "    print(\"Min Reward = \", min_r)\n",
    "    \n",
    "    return R, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "- Procedure Value_Iteration(S,A,P,R,θ):\n",
    "           Inputs\n",
    "                     S is the set of all states\n",
    "                     A is the set of all actions\n",
    "                     P is state transition function specifying P(s'|s,a)\n",
    "                     R is a reward function R(s,a,s')\n",
    "                     θ a threshold, θ>0\n",
    "           Output\n",
    "                     π[S] approximately optimal policy\n",
    "                    V[S] value function\n",
    "           Local\n",
    "                     real array Vk[S] is a sequence of value functions\n",
    "                     action array π[S]\n",
    "           assign V0[S] arbitrarily\n",
    "           k ←0\n",
    "           repeat\n",
    "                     k ←k+1\n",
    "                     for each state s do\n",
    "                               Vk[s] = maxa ∑s' P(s'|s,a) (R(s,a,s')+ γVk-1[s'])\n",
    "           until ∀s |Vk[s]-Vk-1[s]| < θ\n",
    "           for each state s do\n",
    "                     π[s] = argmaxa ∑s' P(s'|s,a) (R(s,a,s')+ γVk[s'])\n",
    "           return π,Vk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_values(env, s, V, gamma=0.99):\n",
    "    action_values = np.zeros(env.nA)\n",
    "    \n",
    "    for a in range(env.nA):\n",
    "        for prob, reward, s_prime in env.P[s][a]:\n",
    "            action_values[a] += prob * ( reward + gamma * V[int(s_prime)])\n",
    "            \n",
    "    return action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Value Iteration'''\n",
    "def value_iteration(env, gamma = 0.999, max_iteration = 1000):\n",
    "    # Initialise Utility Function\n",
    "    V = np.zeros(env.nS)\n",
    "\n",
    "    for i in range(max_iteration):\n",
    "        prev_V = np.copy(V)\n",
    "\n",
    "        #loop over all states\n",
    "        for s in range(env.nS):\n",
    "            action_values = get_action_values(env, s, prev_V, gamma)\n",
    "            best_action_value = np.max(action_values)\n",
    "            V[s] = best_action_value\n",
    "\n",
    "        if i%100 == 0 and np.all(np.isclose(V, prev_V, rtol=0.00)):\n",
    "            print(\"Value converged at iteration \", i)\n",
    "            break\n",
    "\n",
    "    optimal_policy = np.zeros(env.nS, dtype = 'int8')\n",
    "    for s in range(env.nS):\n",
    "        s_action_value = get_action_values(env, s, V, gamma)\n",
    "        optimal_policy[s] = np.argmax(s_action_value)\n",
    "\n",
    "    return V, optimal_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Value Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states:  2\n",
      "Number of actions:  1000\n",
      "Value converged at iteration  18900\n",
      "Time to converge:  2.66e+02 s\n",
      "Optimal Value function: \n",
      "[1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.9999902  1599.9999902  1599.9999902\n",
      " 1599.9999902  1599.9999902  1599.99999023 1599.99999037 1599.99999105\n",
      " 1599.99999445 1600.00001148 1600.0000967  1600.00052325 1600.00265816\n",
      " 1600.01334334 1600.06682277 1600.33448757 1601.67415124 1603.5962878\n",
      " 1606.00136358 1609.01071765 1612.7761757  1617.4877098  1623.38302273\n",
      " 1630.75954041 1639.9894174  1651.53831252 1665.98888199 1684.07017512\n",
      " 1706.69441578 1735.00302521 1770.42420818 1814.7450077  1870.20146355\n",
      " 1939.59142332 2026.41569731 2135.05467877 2270.98934027 2441.07775555\n",
      " 2653.901098   2920.19657153 3253.39911598 3670.31921666 4191.9910143\n",
      " 4844.73350384 5661.47836062 6683.43138462 7962.15138462 9562.15138462]\n",
      "Final Policy: \n",
      "['B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "print(\"Number of states: \", env.action_space.n)\n",
    "print(\"Number of actions: \", env.observation_space.n)\n",
    "\n",
    "start_time = time.time()\n",
    "optimal_value, optimal_policy = value_iteration(env, gamma=0.999, max_iteration=100000 )\n",
    "stop_time = time.time()\n",
    "time_taken = (stop_time - start_time)\n",
    "\n",
    "print (f\"Time to converge: {time_taken : 0.3} s\")\n",
    "\n",
    "print('Optimal Value function: ')\n",
    "print(optimal_value)\n",
    "plt.imshow(optimal_value.reshape(1,N), cmap='gray')\n",
    "\n",
    "print('Final Policy: ')\n",
    "policy = map_actions(optimal_policy)\n",
    "\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of episodes reached end =  0  out of  5000  episodes.\n",
      "Max Reward =  1696.0\n",
      "Avg Reward =  1600.4972\n",
      "Min Reward =  1506.0\n"
     ]
    }
   ],
   "source": [
    "# R, A = get_score(env, optimal_policy, max_timestep=20, episodes=5000)\n",
    "\n",
    "# R, A = get_score(env, optimal_policy, max_timestep=100, episodes=5000)\n",
    "\n",
    "R, A = get_score(env, optimal_policy, max_timestep=1000, episodes=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration\n",
    "The policy iteration algorithm manipulates the policy directly, rather than finding it indirectly via the optimal value function. It operates as follows:\n",
    "\n",
    "<img src='http://incompleteideas.net/book/first/ebook/pseudotmp1.png'>\n",
    "<img src='http://incompleteideas.net/book/first/ebook/imgtmp35.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy_val(env, policy, V, gamma):\n",
    "    policy_values = np.zeros(env.nS)\n",
    "    for s, a in zip(range(len(policy)), policy):\n",
    "        for prob, reward, s_prime in env.P[s][a]:\n",
    "            policy_values[s] += prob * (reward + gamma * V[int(s_prime)])\n",
    "            \n",
    "    return policy_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, gamma = 0.99, max_iteration = 1000):\n",
    "    V = np.zeros(env.nS)\n",
    "    \n",
    "    P = np.random.randint(0, env.nA, env.nS)\n",
    "    prev_P = np.copy(P)\n",
    "    \n",
    "    for i in range(max_iteration):\n",
    "        \n",
    "        V = get_policy_val(env, P, V, gamma)\n",
    "        \n",
    "        for s in range(env.nS):\n",
    "            s_action_value = get_action_values(env, s, V, gamma)\n",
    "            P[s] = np.argmax(s_action_value)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            if np.all(np.equal(P, prev_P)):\n",
    "                print(\"Policy converged at iteration \", i)\n",
    "                break\n",
    "            prev_P = np.copy(P)\n",
    "        \n",
    "    return V, P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### env.reset()\n",
    "print(\"Number of states: \", env.action_space.n)\n",
    "print(\"Number of actions: \", env.observation_space.n)\n",
    "\n",
    "start_time = time.time()\n",
    "optimal_value, optimal_policy = policy_iteration(env, gamma=0.999, max_iteration=1000)\n",
    "stop_time = time.time()\n",
    "time_taken = (stop_time - start_time)\n",
    "\n",
    "print (f\"Time to converge: {time_taken : 0.3} s\")\n",
    "\n",
    "print('Optimal Value function: ')\n",
    "print(optimal_value)\n",
    "\n",
    "print('Final Policy: ')\n",
    "policy = map_actions(optimal_policy)\n",
    "\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of episodes reached end =  0  out of  5000  episodes.\n",
      "Max Reward =  1686.0\n",
      "Avg Reward =  1600.0176\n",
      "Min Reward =  1484.0\n"
     ]
    }
   ],
   "source": [
    "# R, A = get_score(env, optimal_policy, max_timestep=20, episodes=5000)\n",
    "\n",
    "# R, A = get_score(env, optimal_policy, max_timestep=100, episodes=5000)\n",
    "\n",
    "R, A = get_score(env, optimal_policy, max_timestep=1000, episodes=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q- Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "env.reset()\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "\n",
    "qtable = np.zeros((state_size, action_size))\n",
    "T = np.zeros((state_size, action_size, state_size))\n",
    "T_Count = T.copy()\n",
    "R = qtable.copy()\n",
    "\n",
    "total_episodes = 1000000        # Total episodes\n",
    "learning_rate = 0.8           # Learning rate\n",
    "max_steps = 99                # Max steps per episode\n",
    "gamma = 0.99                  # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 152.959952\n",
      "[[117.71896344 123.14686817]\n",
      " [115.51120467 120.86174401]\n",
      " [117.03583756 118.38715033]\n",
      " ...\n",
      " [  0.           0.        ]\n",
      " [  0.           0.        ]\n",
      " [  0.           0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# List of rewards\n",
    "rewards = []\n",
    "\n",
    "# 2 For life or until learning is stopped\n",
    "for episode in range(total_episodes):\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # 3. Choose an action a in the current world state (s)\n",
    "        ## First we randomize a number\n",
    "        exp_exp_tradeoff = random.uniform(0, 1)\n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable[state,:])\n",
    "\n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        # qtable[new_state,:] : all the actions we can take from new state\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "                \n",
    "        total_rewards += reward\n",
    "        \n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "        \n",
    "        # If done (if we're dead) : finish episode\n",
    "        if done == True: \n",
    "            break\n",
    "        \n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value function: \n",
      "[123.147 120.862 118.387 120.867 121.603 116.826 109.665 114.653 115.823\n",
      " 119.416 118.271 114.046 110.76  118.841 113.35   82.403 101.578  62.579\n",
      "  27.33    8.289  76.767  19.251  74.945  56.42   37.881  68.43  109.495\n",
      " 107.319  77.976 112.823   0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.   ]\n",
      "Final Policy: \n",
      "['B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'F' 'B' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimal_policy = np.argmax(qtable, axis=1)\n",
    "optimal_value = np.amax(qtable, axis=1)\n",
    "\n",
    "print('Optimal Value function: ')\n",
    "print(np.round(optimal_value, 3))\n",
    "plt.imshow(optimal_value.reshape((1, N)), cmap='gist_gray_r')\n",
    "\n",
    "print('Final Policy: ')\n",
    "policy = map_actions(optimal_policy)\n",
    "\n",
    "print(np.array(policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of episodes reached end =  0  out of  5000  episodes.\n",
      "Max Reward =  1676.0\n",
      "Avg Reward =  1599.6588\n",
      "Min Reward =  1476.0\n"
     ]
    }
   ],
   "source": [
    "# R, A = get_score(env, optimal_policy, max_timestep=20, episodes=5000)\n",
    "\n",
    "# R, A = get_score(env, optimal_policy, max_timestep=100, episodes=5000)\n",
    "\n",
    "R, A = get_score(env, optimal_policy, max_timestep=1000, episodes=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import random\n",
    "env.reset()\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "\n",
    "qtable = np.zeros((state_size, action_size))\n",
    "T = np.zeros((state_size, action_size, state_size))\n",
    "T_Count = T.copy()\n",
    "R = qtable.copy()\n",
    "\n",
    "total_episodes = 10000        # Total episodes\n",
    "learning_rate = 0.8           # Learning rate\n",
    "max_steps = 99                # Max steps per episode\n",
    "gamma = 0.95                  # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of rewards\n",
    "rewards = []\n",
    "\n",
    "# 2 For life or until learning is stopped\n",
    "for episode in range(total_episodes):\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # 3. Choose an action a in the current world state (s)\n",
    "        ## First we randomize a number\n",
    "        exp_exp_tradeoff = random.uniform(0, 1)\n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable[state,:])\n",
    "\n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        # qtable[new_state,:] : all the actions we can take from new state\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "                \n",
    "        total_rewards += reward\n",
    "        \n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "        \n",
    "        # If done (if we're dead) : finish episode\n",
    "        if done == True: \n",
    "            break\n",
    "        \n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimal_policy = np.argmax(qtable, axis=1)\n",
    "optimal_value = np.amax(qtable, axis=1)\n",
    "\n",
    "print('Optimal Value function: ')\n",
    "print(np.round(optimal_value, 3))\n",
    "plt.imshow(optimal_value.reshape((1, N)), cmap='gist_gray_r')\n",
    "\n",
    "print('Final Policy: ')\n",
    "policy = map_actions(optimal_policy)\n",
    "\n",
    "print(np.array(policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R, A = get_score(env, optimal_policy, max_timestep=20, episodes=5000)\n",
    "\n",
    "R, A = get_score(env, optimal_policy, max_timestep=100, episodes=5000)\n",
    "\n",
    "R, A = get_score(env, optimal_policy, max_timestep=500, episodes=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
