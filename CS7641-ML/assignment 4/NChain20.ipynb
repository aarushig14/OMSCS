{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import gym.spaces as spaces\n",
    "import gym.envs as envs\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting of Environment\n",
    "N = 20\n",
    "env = gym.make('NChain-v0', n=N, large=40, small=2, slip=0.2)\n",
    "\n",
    "env.nS = env.observation_space.n\n",
    "env.nA = env.action_space.n\n",
    "\n",
    "# P[state][action] = prob, reward, s_prime\n",
    "env.P = np.zeros((env.nS, env.nA, env.nA, 3)) \n",
    "for s in range(env.nS):\n",
    "    if s == env.nS - 1 :\n",
    "        env.P[s][0] = [ [(1-env.slip), env.large, s], [env.slip, env.small, 0] ]\n",
    "        env.P[s][1] = [ [env.slip, env.large, s], [1 - env.slip, env.small, 0] ]\n",
    "        continue\n",
    "    # Forward\n",
    "    env.P[s][0] = [ [(1-env.slip), 0 if s < env.nS - 1 else env.large, s+1], [env.slip, env.small, 0] ]\n",
    "    # Backward\n",
    "    env.P[s][1] = [ [env.slip, 0 if s < env.nS - 1 else env.large, s+1], [1 - env.slip, env.small, 0] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_actions(optimal_policy):\n",
    "    policy = []\n",
    "    for i in optimal_policy:\n",
    "        if i == 0: # Forward\n",
    "            policy.append('F')\n",
    "        elif i == 1: # Backward\n",
    "            policy.append('B')\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Analysis\n",
    "\n",
    "def get_score(env, policy, max_timestep=200, episodes=1000):\n",
    "    R = np.zeros((episodes, max_timestep))\n",
    "    A = np.zeros((episodes, max_timestep))\n",
    "    ended = 0\n",
    "    reached = []\n",
    "    for ep in range(episodes):\n",
    "        s = env.reset()\n",
    "        for i in range(max_timestep):\n",
    "            a = policy[s]\n",
    "            A[ep][i] = a\n",
    "            s_prime, reward, _, _ = env.step(a)\n",
    "            R[ep][i] = reward\n",
    "            s = s_prime\n",
    "        if env.large in R[ep]:\n",
    "            ended += 1\n",
    "            reached.append(ep)\n",
    "    \n",
    "    total_R = np.sum(R, axis=1)\n",
    "    max_r = np.max(total_R)\n",
    "    min_r = np.min(total_R)\n",
    "    avg_r = np.mean(total_R)\n",
    "    print(\"Number of episodes reached end = \", ended, \" out of \", episodes, \" episodes.\" )\n",
    "    if len(reached) > 0:\n",
    "        print(\"Max reward where episode reached end = \", np.max(total_R[reached]))\n",
    "        print(\"Min reward where episode reached end = \", np.min(total_R[reached]))\n",
    "    print(\"Max Reward = \", max_r)\n",
    "    print(\"Avg Reward = \", avg_r)\n",
    "    print(\"Min Reward = \", min_r)\n",
    "    \n",
    "    return R, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "- Procedure Value_Iteration(S,A,P,R,θ):\n",
    "           Inputs\n",
    "                     S is the set of all states\n",
    "                     A is the set of all actions\n",
    "                     P is state transition function specifying P(s'|s,a)\n",
    "                     R is a reward function R(s,a,s')\n",
    "                     θ a threshold, θ>0\n",
    "           Output\n",
    "                     π[S] approximately optimal policy\n",
    "                    V[S] value function\n",
    "           Local\n",
    "                     real array Vk[S] is a sequence of value functions\n",
    "                     action array π[S]\n",
    "           assign V0[S] arbitrarily\n",
    "           k ←0\n",
    "           repeat\n",
    "                     k ←k+1\n",
    "                     for each state s do\n",
    "                               Vk[s] = maxa ∑s' P(s'|s,a) (R(s,a,s')+ γVk-1[s'])\n",
    "           until ∀s |Vk[s]-Vk-1[s]| < θ\n",
    "           for each state s do\n",
    "                     π[s] = argmaxa ∑s' P(s'|s,a) (R(s,a,s')+ γVk[s'])\n",
    "           return π,Vk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_values(env, s, V, gamma=0.99):\n",
    "    action_values = np.zeros(env.nA)\n",
    "    \n",
    "    for a in range(env.nA):\n",
    "        for prob, reward, s_prime in env.P[s][a]:\n",
    "            action_values[a] += prob * ( reward + gamma * V[int(s_prime)])\n",
    "            \n",
    "    return action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Value Iteration'''\n",
    "def value_iteration(env, gamma = 0.999, max_iteration = 1000):\n",
    "    # Initialise Utility Function\n",
    "    V = np.zeros(env.nS)\n",
    "\n",
    "    for i in range(max_iteration):\n",
    "        prev_V = np.copy(V)\n",
    "\n",
    "        #loop over all states\n",
    "        for s in range(env.nS):\n",
    "            action_values = get_action_values(env, s, prev_V, gamma)\n",
    "            best_action_value = np.max(action_values)\n",
    "            V[s] = best_action_value\n",
    "\n",
    "        if i%10 == 0 and np.all(np.isclose(V, prev_V)):\n",
    "            print(\"Value converged at iteration \", i)\n",
    "            break\n",
    "\n",
    "    optimal_policy = np.zeros(env.nS, dtype = 'int8')\n",
    "    for s in range(env.nS):\n",
    "        s_action_value = get_action_values(env, s, V, gamma)\n",
    "        optimal_policy[s] = np.argmax(s_action_value)\n",
    "\n",
    "    return V, optimal_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Value Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states:  2\n",
      "Number of actions:  20\n",
      "Value converged at iteration  4620\n",
      "Time to converge:  1.5e+03 ms\n",
      "Optimal Value function: \n",
      "[1585. 1585. 1585. 1585. 1585. 1585. 1587. 1589. 1592. 1595. 1600. 1605.\n",
      " 1612. 1620. 1631. 1644. 1660. 1680. 1706. 1738.]\n",
      "Final Policy: \n",
      "['B', 'B', 'B', 'B', 'B', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "print(\"Number of states: \", env.action_space.n)\n",
    "print(\"Number of actions: \", env.observation_space.n)\n",
    "\n",
    "start_time = time.time()\n",
    "optimal_value, optimal_policy = value_iteration(env, gamma=0.999, max_iteration=10000 )\n",
    "stop_time = time.time()\n",
    "time_taken = (stop_time - start_time)*1000\n",
    "\n",
    "print (f\"Time to converge: {time_taken : 0.3} ms\")\n",
    "\n",
    "print('Optimal Value function: ')\n",
    "print(np.round(optimal_value))\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(optimal_value.reshape(1,N), cmap='Oranges_r')\n",
    "\n",
    "print('Final Policy: ')\n",
    "policy = map_actions(optimal_policy)\n",
    "\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of episodes reached end =  0  out of  5000  episodes.\n",
      "Max Reward =  40.0\n",
      "Avg Reward =  31.9608\n",
      "Min Reward =  8.0\n",
      "Number of episodes reached end =  5  out of  5000  episodes.\n",
      "Max reward where episode reached end =  668.0\n",
      "Min reward where episode reached end =  196.0\n",
      "Max Reward =  668.0\n",
      "Avg Reward =  160.1008\n",
      "Min Reward =  128.0\n",
      "Number of episodes reached end =  22  out of  5000  episodes.\n",
      "Max reward where episode reached end =  1350.0\n",
      "Min reward where episode reached end =  812.0\n",
      "Max Reward =  1350.0\n",
      "Avg Reward =  800.4296\n",
      "Min Reward =  738.0\n"
     ]
    }
   ],
   "source": [
    "R, A = get_score(env, optimal_policy, max_timestep=20, episodes=5000)\n",
    "\n",
    "R, A = get_score(env, optimal_policy, max_timestep=100, episodes=5000)\n",
    "\n",
    "R, A = get_score(env, optimal_policy, max_timestep=500, episodes=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration\n",
    "The policy iteration algorithm manipulates the policy directly, rather than finding it indirectly via the optimal value function. It operates as follows:\n",
    "\n",
    "<img src='http://incompleteideas.net/book/first/ebook/pseudotmp1.png'>\n",
    "<img src='http://incompleteideas.net/book/first/ebook/imgtmp35.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy_val(env, policy, V, gamma):\n",
    "    policy_values = np.zeros(env.nS)\n",
    "    for s, a in zip(range(len(policy)), policy):\n",
    "        for prob, reward, s_prime in env.P[s][a]:\n",
    "            policy_values[s] += prob * (reward + gamma * V[int(s_prime)])\n",
    "            \n",
    "    return policy_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, gamma = 0.99, max_iteration = 1000):\n",
    "    V = np.zeros(env.nS)\n",
    "    \n",
    "    P = np.random.randint(0, env.nA, env.nS)\n",
    "    prev_P = np.copy(P)\n",
    "    \n",
    "    for i in range(max_iteration):\n",
    "        \n",
    "        V = get_policy_val(env, P, V, gamma)\n",
    "        \n",
    "        for s in range(env.nS):\n",
    "            s_action_value = get_action_values(env, s, V, gamma)\n",
    "            P[s] = np.argmax(s_action_value)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            if np.all(np.equal(P, prev_P)):\n",
    "                print(\"Policy converged at iteration \", i)\n",
    "                break\n",
    "            prev_P = np.copy(P)\n",
    "        \n",
    "    return V, P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states:  2\n",
      "Number of actions:  20\n",
      "Policy converged at iteration  40\n",
      "Time to converge:  19.4 ms\n",
      "Optimal Value function: \n",
      "[ 63.  63.  63.  63.  63.  64.  66.  68.  71.  74.  78.  84.  90.  99.\n",
      " 109. 122. 139. 159. 185. 217.]\n",
      "Final Policy: \n",
      "['B', 'B', 'B', 'B', 'B', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "print(\"Number of states: \", env.action_space.n)\n",
    "print(\"Number of actions: \", env.observation_space.n)\n",
    "\n",
    "start_time = time.time()\n",
    "optimal_value, optimal_policy = policy_iteration(env, gamma=0.999, max_iteration=1000)\n",
    "stop_time = time.time()\n",
    "time_taken = (stop_time - start_time)*1000\n",
    "\n",
    "print (f\"Time to converge: {time_taken : 0.3} ms\")\n",
    "\n",
    "print('Optimal Value function: ')\n",
    "print(np.round(optimal_value))\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(optimal_value.reshape(1,N), cmap='Oranges_r')\n",
    "\n",
    "print('Final Policy: ')\n",
    "policy = map_actions(optimal_policy)\n",
    "\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of episodes reached end =  0  out of  5000  episodes.\n",
      "Max Reward =  40.0\n",
      "Avg Reward =  32.0096\n",
      "Min Reward =  10.0\n",
      "Number of episodes reached end =  5  out of  5000  episodes.\n",
      "Max reward where episode reached end =  472.0\n",
      "Min reward where episode reached end =  178.0\n",
      "Max Reward =  472.0\n",
      "Avg Reward =  159.8324\n",
      "Min Reward =  122.0\n",
      "Number of episodes reached end =  23  out of  5000  episodes.\n",
      "Max reward where episode reached end =  1512.0\n",
      "Min reward where episode reached end =  794.0\n",
      "Max Reward =  1512.0\n",
      "Avg Reward =  800.3208\n",
      "Min Reward =  726.0\n"
     ]
    }
   ],
   "source": [
    "R, A = get_score(env, optimal_policy, max_timestep=20, episodes=5000)\n",
    "\n",
    "R, A = get_score(env, optimal_policy, max_timestep=100, episodes=5000)\n",
    "\n",
    "R, A = get_score(env, optimal_policy, max_timestep=500, episodes=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q- Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "env.reset()\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "\n",
    "qtable = np.zeros((state_size, action_size))\n",
    "T = np.zeros((state_size, action_size, state_size))\n",
    "T_Count = T.copy()\n",
    "R = qtable.copy()\n",
    "\n",
    "total_episodes = 10000        # Total episodes\n",
    "learning_rate = 0.8           # Learning rate\n",
    "max_steps = 99                # Max steps per episode\n",
    "gamma = 0.95                  # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time  12.102376937866211\n",
      "Score over time: 151.0362\n",
      "[[18.5991882  18.87505395]\n",
      " [19.46672728 19.1069552 ]\n",
      " [18.99909671 20.12553711]\n",
      " [17.93899699 19.89096854]\n",
      " [18.46391637 21.09006579]\n",
      " [18.42295787 23.37230881]\n",
      " [18.70645477 22.20475272]\n",
      " [19.3554489  22.21718057]\n",
      " [19.84933976 19.63613767]\n",
      " [23.88747405 19.8475119 ]\n",
      " [ 0.         24.15621992]\n",
      " [ 0.         25.10969   ]\n",
      " [26.67749487  0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# List of rewards\n",
    "rewards = []\n",
    "start_time = time.time()\n",
    "# 2 For life or until learning is stopped\n",
    "for episode in range(total_episodes):\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # 3. Choose an action a in the current world state (s)\n",
    "        ## First we randomize a number\n",
    "        exp_exp_tradeoff = random.uniform(0, 1)\n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable[state,:])\n",
    "\n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        # qtable[new_state,:] : all the actions we can take from new state\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "                \n",
    "        total_rewards += reward\n",
    "        \n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "        \n",
    "        # If done (if we're dead) : finish episode\n",
    "        if done == True: \n",
    "            break\n",
    "        \n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "    rewards.append(total_rewards)\n",
    "stop_time = time.time()\n",
    "print(\"Time \", stop_time - start_time)\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 2)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qtable.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value function: \n",
      "[18.875 19.467 20.126 19.891 21.09  23.372 22.205 22.217 19.849 23.887\n",
      " 24.156 25.11  26.677  0.     0.     0.     0.     0.     0.     0.   ]\n",
      "Final Policy: \n",
      "['B' 'F' 'B' 'B' 'B' 'B' 'B' 'B' 'F' 'F' 'B' 'B' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimal_policy = np.argmax(qtable, axis=1)\n",
    "optimal_value = np.amax(qtable, axis=1)\n",
    "\n",
    "print('Optimal Value function: ')\n",
    "print(np.round(optimal_value, 3))\n",
    "plt.imshow(optimal_value.reshape((1, N)), cmap='gist_gray_r')\n",
    "\n",
    "print('Final Policy: ')\n",
    "policy = map_actions(optimal_policy)\n",
    "\n",
    "print(np.array(policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of episodes reached end =  0  out of  5000  episodes.\n",
      "Max Reward =  40.0\n",
      "Avg Reward =  28.6792\n",
      "Min Reward =  12.0\n",
      "Number of episodes reached end =  0  out of  5000  episodes.\n",
      "Max Reward =  180.0\n",
      "Avg Reward =  143.1012\n",
      "Min Reward =  108.0\n",
      "Number of episodes reached end =  0  out of  5000  episodes.\n",
      "Max Reward =  806.0\n",
      "Avg Reward =  714.8244\n",
      "Min Reward =  636.0\n"
     ]
    }
   ],
   "source": [
    "R, A = get_score(env, optimal_policy, max_timestep=20, episodes=5000)\n",
    "\n",
    "R, A = get_score(env, optimal_policy, max_timestep=100, episodes=5000)\n",
    "\n",
    "R, A = get_score(env, optimal_policy, max_timestep=500, episodes=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
