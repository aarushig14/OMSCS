{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import gym.spaces as spaces\n",
    "import gym.envs as envs\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Analysis\n",
    "\n",
    "def get_score(env, policy, episodes=1000):\n",
    "    wins = 0\n",
    "    loses = 0\n",
    "    total_reward = 0\n",
    "    steps = []\n",
    "    sequence = []\n",
    "    min_steps = np.Inf\n",
    "    \n",
    "    for i in range(episodes):\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        cnt = 0\n",
    "        seq_actions = []\n",
    "        while True:\n",
    "            action = policy[state]\n",
    "            seq_actions.append(action)\n",
    "\n",
    "            s_prime, r, done, _ = env.step(action)\n",
    "            \n",
    "            total_reward += r\n",
    "            cnt += 1\n",
    "            state = s_prime\n",
    "            \n",
    "            if done and r == 1.0:\n",
    "                wins += 1\n",
    "                steps.append(cnt)\n",
    "                if cnt < min_steps:\n",
    "                    min_steps = cnt\n",
    "                    sequence = seq_actions\n",
    "                break;\n",
    "            elif done and r == 0.0:\n",
    "                loses += 1\n",
    "                break;\n",
    "    \n",
    "    print(\"Won Percentage = \", (wins/episodes) * 100)\n",
    "    print(\"Lost Percentage = \", (loses/episodes) * 100)\n",
    "    print(\"Average steps taken to win = \", np.mean(steps))\n",
    "    print(\"Average reward in all episodes = \", total_reward/episodes)\n",
    "    \n",
    "    return wins, loses, steps, sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_actions(optimal_policy):\n",
    "    policy = []\n",
    "    for i in optimal_policy:\n",
    "        if i == 0: # LEFT\n",
    "            policy.append('\\u2190')\n",
    "        elif i == 1: # DOWN\n",
    "            policy.append('\\u2193')\n",
    "        elif i == 2: # RIGHT\n",
    "            policy.append('\\u2192')\n",
    "        elif i == 3: # UP\n",
    "            policy.append('\\u2191')\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "- Procedure Value_Iteration(S,A,P,R,θ):\n",
    "           Inputs\n",
    "                     S is the set of all states\n",
    "                     A is the set of all actions\n",
    "                     P is state transition function specifying P(s'|s,a)\n",
    "                     R is a reward function R(s,a,s')\n",
    "                     θ a threshold, θ>0\n",
    "           Output\n",
    "                     π[S] approximately optimal policy\n",
    "                    V[S] value function\n",
    "           Local\n",
    "                     real array Vk[S] is a sequence of value functions\n",
    "                     action array π[S]\n",
    "           assign V0[S] arbitrarily\n",
    "           k ←0\n",
    "           repeat\n",
    "                     k ←k+1\n",
    "                     for each state s do\n",
    "                               Vk[s] = maxa ∑s' P(s'|s,a) (R(s,a,s')+ γVk-1[s'])\n",
    "           until ∀s |Vk[s]-Vk-1[s]| < θ\n",
    "           for each state s do\n",
    "                     π[s] = argmaxa ∑s' P(s'|s,a) (R(s,a,s')+ γVk[s'])\n",
    "           return π,Vk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_values(env, s, V, gamma=0.99):\n",
    "    action_values = np.zeros(env.nA)\n",
    "    \n",
    "    for a in range(env.nA):\n",
    "        for prob, s_prime, r, _ in env.P[s][a]:\n",
    "            action_values[a] += prob * ( r + gamma * V[s_prime])\n",
    "            \n",
    "    return action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Value Iteration'''\n",
    "def value_iteration(env, gamma = 0.999, max_iteration = 1000):\n",
    "    # Initialise Utility Function\n",
    "    V = np.zeros(env.nS)\n",
    "\n",
    "    for i in range(max_iteration):\n",
    "        prev_V = np.copy(V)\n",
    "\n",
    "        #loop over all states\n",
    "        for s in range(env.nS):\n",
    "            action_values = get_action_values(env, s, prev_V, gamma)\n",
    "            best_action_value = np.max(action_values)\n",
    "            V[s] = best_action_value\n",
    "\n",
    "        if i % 5 == 0 and np.all(np.isclose(V, prev_V)):\n",
    "            print(\"Value converged at iteration \", i)\n",
    "            break\n",
    "\n",
    "    optimal_policy = np.zeros(env.nS, dtype = 'int8')\n",
    "    for s in range(env.nS):\n",
    "        s_action_value = get_action_values(env, s, V, gamma)\n",
    "        optimal_policy[s] = np.argmax(s_action_value)\n",
    "\n",
    "    return V, optimal_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Value Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states:  16\n",
      "Number of actions:  4\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Value converged at iteration  335\n",
      "Time to converge:  93.2 ms\n",
      "Optimal Value function: \n",
      "[[0.785 0.778 0.774 0.771]\n",
      " [0.788 0.    0.506 0.   ]\n",
      " [0.792 0.8   0.745 0.   ]\n",
      " [0.    0.864 0.931 0.   ]]\n",
      "Final Policy: \n",
      "[['←' '↑' '↑' '↑']\n",
      " ['←' '←' '←' '←']\n",
      " ['↑' '↓' '←' '←']\n",
      " ['←' '→' '↓' '←']]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "print(\"Number of states: \", env.nS)\n",
    "print(\"Number of actions: \", env.nA)\n",
    "\n",
    "env.render()\n",
    "\n",
    "start_time = time.time()\n",
    "optimal_value, optimal_policy = value_iteration(env.env, gamma=0.999, max_iteration=1000 )\n",
    "stop_time = time.time()\n",
    "time_taken = (stop_time - start_time)*1000\n",
    "\n",
    "print (f\"Time to converge: {time_taken : 0.3} ms\")\n",
    "\n",
    "print('Optimal Value function: ')\n",
    "print(np.round(optimal_value, 3).reshape((4, 4)))\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(optimal_value.reshape((4, 4)), cmap='Oranges_r')\n",
    "\n",
    "print('Final Policy: ')\n",
    "policy = map_actions(optimal_policy)\n",
    "\n",
    "print(np.array(policy).reshape((4,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Won Percentage =  74.9\n",
      "Lost Percentage =  25.1\n",
      "Average steps taken to win =  38.308411214953274\n",
      "Average reward in all episodes =  0.749\n",
      "Sequence of min steps taken:  ['←', '↑', '↑', '↑', '←', '←', '←', '←', '↑', '↓', '←', '←', '←', '→', '↓', '←']\n",
      "Min steps to win:  6\n"
     ]
    }
   ],
   "source": [
    "wins, loses, steps, sequence = get_score(env, optimal_policy, episodes=1000)\n",
    "print(\"Sequence of min steps taken: \", map_actions(sequence))\n",
    "print(\"Min steps to win: \", len(sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration\n",
    "The policy iteration algorithm manipulates the policy directly, rather than finding it indirectly via the optimal value function. It operates as follows:\n",
    "\n",
    "<img src='http://incompleteideas.net/book/first/ebook/pseudotmp1.png'>\n",
    "<img src='http://incompleteideas.net/book/first/ebook/imgtmp35.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy_val(env, policy, V, gamma):\n",
    "    policy_values = np.zeros(env.nS)\n",
    "    for s, a in zip(range(len(policy)), policy):\n",
    "        for prob, s_prime, r, _ in env.P[s][a]:\n",
    "            policy_values[s] += prob * ( r + gamma * V[s_prime])\n",
    "            \n",
    "    return policy_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, gamma = 0.99, max_iteration = 1000):\n",
    "    V = np.zeros(env.nS)\n",
    "    \n",
    "    P = np.random.randint(0, 4, env.nS)\n",
    "    prev_P = np.copy(P)\n",
    "    \n",
    "    for i in range(max_iteration):\n",
    "        \n",
    "        V = get_policy_val(env, P, V, gamma)\n",
    "        \n",
    "        for s in range(env.nS):\n",
    "            s_action_value = get_action_values(env, s, V, gamma)\n",
    "            P[s] = np.argmax(s_action_value)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            if np.all(np.equal(P, prev_P)):\n",
    "                print(\"Policy converged at iteration \", i)\n",
    "                break\n",
    "            prev_P = np.copy(P)\n",
    "        \n",
    "    return V, P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### env2 = gym.make('FrozenLake-v0')\n",
    "print(\"Number of states: \", env2.nS)\n",
    "print(\"Number of actions: \", env2.nA)\n",
    "\n",
    "env2.render()\n",
    "\n",
    "start_time2 = time.time()\n",
    "optimal_value2, optimal_policy2 = policy_iteration(env2.env, gamma=0.999, max_iteration=1000)\n",
    "stop_time2 = time.time()\n",
    "time_taken2 = (stop_time2 - start_time2) * 1000\n",
    "\n",
    "print (f\"Time to converge: {time_taken2 : 0.3} ms\")\n",
    "\n",
    "print('Optimal Value function: ')\n",
    "print(np.round(optimal_value2, 3).reshape((4, 4)))\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(optimal_value2.reshape((4, 4)), cmap='Oranges_r')\n",
    "\n",
    "print('Final Policy: ')\n",
    "policy2 = map_actions(optimal_policy2)\n",
    "\n",
    "print(np.array(policy2).reshape((4,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Won Percentage =  73.8\n",
      "Lost Percentage =  26.200000000000003\n",
      "Average steps taken to win =  37.56910569105691\n",
      "Average reward in all episodes =  0.738\n",
      "Sequence of min steps taken:  ['←', '↑', '↑', '↑', '←', '←', '←', '←', '↑', '↓', '←', '←', '←', '→', '↓', '←']\n",
      "Min steps to win:  6\n"
     ]
    }
   ],
   "source": [
    "wins2, loses2, steps2, sequence2 = get_score(env2, optimal_policy2, episodes=1000)\n",
    "print(\"Sequence of min steps taken: \", map_actions(sequence2))\n",
    "print(\"Min steps to win: \", len(sequence2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5333798903627254\n",
      "0.3101127456459144\n",
      "0.3125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ True,  True, False,  True],\n",
       "       [ True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(optimal_value.mean())\n",
    "print(optimal_value2.mean())\n",
    "print(len(np.where(optimal_value<=0.3)[0])/16)\n",
    "(optimal_policy == optimal_policy2).reshape((4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q- Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import random as rand\n",
    "env3 = gym.make('FrozenLake-v0')\n",
    "action_size = env3.action_space.n\n",
    "state_size = env3.observation_space.n\n",
    "\n",
    "qtable = np.zeros((state_size, action_size))\n",
    "T = np.zeros((state_size, action_size, state_size))\n",
    "T_Count = T.copy()\n",
    "R = qtable.copy()\n",
    "\n",
    "total_episodes = 100000        # Total episodes\n",
    "learning_rate = 0.8           # Learning rate\n",
    "max_steps = 99                # Max steps per episode\n",
    "gamma = 0.95                  # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.34105\n",
      "Time taken:  56.08213996887207  s.\n",
      "[[4.58869585e-02 4.63198447e-01 4.65946075e-02 5.87087869e-02]\n",
      " [1.52177911e-04 1.53134331e-02 4.65081914e-04 2.39507947e-01]\n",
      " [1.46571877e-02 1.77730994e-03 4.57840318e-01 2.13433403e-02]\n",
      " [4.51596204e-03 1.79628926e-03 1.91378534e-02 9.10966733e-02]\n",
      " [1.94231408e-01 2.62701186e-04 1.27238612e-02 3.49117862e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.70822632e-04 5.42621156e-05 9.70650024e-02 6.15396670e-06]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [3.80602096e-02 3.30168576e-03 2.81900053e-02 3.84971772e-01]\n",
      " [4.93386983e-04 7.98984608e-01 9.50425459e-03 6.10996064e-03]\n",
      " [6.67908466e-01 2.98995042e-02 1.14842317e-03 2.69270070e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.94873539e-01 4.28104822e-02 9.34198851e-01 1.02886501e-01]\n",
      " [2.07744937e-01 9.76838564e-01 2.75411061e-01 2.17651553e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# List of rewards\n",
    "rewards = []\n",
    "start_time = time.time()\n",
    "\n",
    "# 2 For life or until learning is stopped\n",
    "for episode in range(total_episodes):\n",
    "    # Reset the environment\n",
    "    state = env3.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # 3. Choose an action a in the current world state (s)\n",
    "        ## First we randomize a number\n",
    "        exp_exp_tradeoff = random.uniform(0, 1)\n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable[state,:])\n",
    "\n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = env3.action_space.sample()\n",
    "\n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env3.step(action)\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        # qtable[new_state,:] : all the actions we can take from new state\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "                \n",
    "        total_rewards += reward\n",
    "        \n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "        \n",
    "        # If done (if we're dead) : finish episode\n",
    "        if done == True: \n",
    "            break\n",
    "        \n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "stop_time = time.time()\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
    "print (\"Time taken: \", (stop_time - start_time), \" s.\")\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value function: \n",
      "[[0.463 0.24  0.458 0.091]\n",
      " [0.194 0.    0.097 0.   ]\n",
      " [0.385 0.799 0.668 0.   ]\n",
      " [0.    0.934 0.977 0.   ]]\n",
      "Final Policy: \n",
      "[['↓' '↑' '→' '↑']\n",
      " ['←' '←' '→' '←']\n",
      " ['↑' '↓' '←' '←']\n",
      " ['←' '→' '↓' '←']]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimal_policy3 = np.argmax(qtable, axis=1)\n",
    "optimal_value3 = np.amax(qtable, axis=1)\n",
    "\n",
    "print('Optimal Value function: ')\n",
    "print(np.round(optimal_value3, 3).reshape((4, 4)))\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(optimal_value3.reshape((4, 4)), cmap='Oranges_r')\n",
    "\n",
    "print('Final Policy: ')\n",
    "policy3 = map_actions(optimal_policy3)\n",
    "\n",
    "print(np.array(policy3).reshape((4,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Won Percentage =  46.800000000000004\n",
      "Lost Percentage =  53.2\n",
      "Average steps taken to win =  28.48931623931624\n",
      "Average reward in all episodes =  0.468\n",
      "Sequence of min steps taken:  ['↓', '↑', '→', '→', '←', '↓']\n",
      "Min steps to win:  6\n"
     ]
    }
   ],
   "source": [
    "wins3, loses3, steps3, sequence3 = get_score(env3, optimal_policy3, episodes=1000)\n",
    "print(\"Sequence of min steps taken: \", map_actions(sequence3))\n",
    "print(\"Min steps to win: \", len(sequence3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(np.round(optimal_value3, 3).reshape((4,4))).to_csv('b.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
